---
title: "hw5"
author: "Xiangxiong Feng"
date: "2023-11-15"
output: html_document
---





Problem 2


```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(purrr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))


```

```{r}
zip_path = 
  tibble(
    path = 
      list.files(path = 'zip_file' ,pattern = "\\.csv$", full.names = TRUE))

```


```{r}
zip_name = 
  tibble(
    file_name = list.files(path = 'zip_file')
    )|>
  bind_cols(zip_path)

zip_name = 
  zip_name|>
  mutate(file = map(zip_name$path, read.csv))|>
  unnest(file)|>
  pivot_longer(
    week_1:week_8,
    names_to = 'time',
    values_to = 'value'
  )
  
  
  
  
```


  
  
  
  
  
  mutate(data = map(zip_name$path, read.csv))|>
  unnest(data)|>
  pivot_longer(
    week_1:week_8,
    names_to = 'time',
    values_to = 'value'
  )|>
  select(-path)
```









Problem 3

# association between effect size and test power

```{r}
simulate_test = function(mu, n=30, sigma =5, alpha = 0.05) {
  data =
    rnorm(n, mean = mu, sd = sigma)
  
  t_result = 
    t.test(data, alpha = 0.05)|>
    broom::tidy()
  
  estimate = t_result$estimate
  p_value = t_result$p.value
  
  table =
    tibble(
      mu_hat = estimate,
      p_value = p_value,
    )
  
  table
}





```

```{r}
sim_results_df = 
  expand_grid(
    mu = c(1,2,3,4,5,6),
    iter = 1:5000
  ) |> 
  mutate(
    results = map(mu, simulate_test)
  ) |> 
  unnest(results)|>
  mutate(test_power = case_when(
    p_value <= 0.05 ~ 'rejected',
    p_value > 0.05 ~ 'failed'
  ))


```

```{r}
sim_results_df|>
  group_by(test_power, mu)|>
  filter(test_power == 'rejected')|>
  summarise(n_obs = n())|>
  mutate(rejected_proportion = n_obs/sum(n_obs))|>
  ggplot(aes(x = mu, y = rejected_proportion)) + geom_point()+geom_line() + ggtitle('association between effect size and test power')
```


The proportion of 'rejected' (the power of the test) is increasing with a larger effect size.




# relation between average estimated mu and true mu
```{r}
sim_results_df|>
  group_by(mu)|>
  summarise(mean_mu = mean(mu_hat))|>
  ggplot(aes(x = mu, y = mean_mu)) + geom_point()+geom_line()+ggtitle('average estimated mu vs. true mu')
  
  
  
  
  
```




```{r}
sim_results_df|>
  filter(test_power == 'rejected')|>
  group_by(mu)|>
  summarise(mean_mu = mean(mu_hat))|>
  ggplot(aes(x = mu, y = mean_mu)) + geom_point()+geom_line()+ggtitle('average estimated mu when null is rejected vs. true mu')
  
  
```
  
From two plots above, we can see that the sample average estimated mu for which null is rejected only approximately equal to the true mu when the true mu is relatively large (>4).
This is because a larger effect size can increase the power of the test.

